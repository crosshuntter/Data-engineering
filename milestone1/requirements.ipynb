{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load dataset\n",
    "- Explore the dataset and ask atleast 5 questions to give you a better understanding of the data provided to you. \n",
    "- Visualise the answer to these 5 questions.\n",
    "- Cleaning the data\n",
    "    - Tidy up the column names, make sure there is no spaces\n",
    "    - Observe,comment on and handle inconsistent data.(i.e duplicates, irrelevant data,incorrect data,etc)\n",
    "    - Observe missing data and comment on why you believe it is missing(MCAR,MAR or MNAR).\n",
    "    - Handle missing data\n",
    "    - Observe and comment on outliers\n",
    "    - Handle outliers\n",
    "- With every change you are making to the data you need to comment on why you used this technique and how has it affected the data(by both showing the change in the data i.e change in number of rows/columns,change in distrubution, etc and commenting on it).\n",
    "- Data transformation and feature engineering\n",
    "    - Add 2 new columns named 'Week number' and 'Date range' and discretize the data into weeks according to the dates. \n",
    "        - Tip: Change the datatype of the date feature to datetime type instead of object.\n",
    "    - Encode any categorical feature(s) and comment on why you used this technique and how the data has changed.\n",
    "    - If exists , Identify feature(s) which need normalization and show your reasoning. Then choose a technique to normalize the feature(s) and comment on why you chose this technique.\n",
    "- Additional data extraction\n",
    "    - Add GPS coordinates for the cities/locations.\n",
    "    - For this task you can extract the GPS coordinates from an API or web scraping and integrate into your csv file as new features. \n",
    "    - Tip 1 - you can find the web scraping and data integration notebooks under 'additional resources'  on the CMS useful.\n",
    "    - Tip 2 - If you are going to use an API make sure you do not make request for each existing row but rather group by the cities and get their respective coordinates. Making a request for each row is too inefficient and expensive.\n",
    "    - Tip 3 - Rather than running the code for calling the API each time you load the notebook, the first time you call the API save the results in a csv file and then you could you check if a csv file exists for the GPS coordinates, if so, load directly and don't call APi. Same applies for web scraping.\n",
    "\n",
    "- Lookup table and load back into new csv file\n",
    "    - Create a lookup table\n",
    "    - Load the new dataset into a new csv file named `green_trip_data_{year}-{month}clean.csv`. replace year and month with the appropriate values.\n",
    "    - Load the lookup table to a csv file called `lookup_table_green_taxis.csv` \n",
    "- Bonus: Load the dataset as a parquet file instead of a csv file(Parquet file is a compressed file format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
