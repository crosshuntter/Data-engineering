{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# For min_max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For z-score scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For Label Encoding\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df = pd.read_csv('../dataset/green_tripdata_2018-05.csv')\n",
    "green_taxi_df_clean = green_taxi_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "#     make all cols lower case\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    df.columns = [col.replace(' ', '_') for col in df.columns]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns(green_taxi_df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe inconsistent data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean = green_taxi_df_clean.drop_duplicates(subset=['lpep_pickup_datetime', 'lpep_dropoff_datetime','pu_location', 'do_location','trip_distance'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### irrelevant or incorrect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only trips that happen in 2018\n",
    "green_taxi_df_clean['lpep_pickup_datetime'] = pd.to_datetime(green_taxi_df_clean['lpep_pickup_datetime'])\n",
    "green_taxi_df_clean['lpep_dropoff_datetime'] = pd.to_datetime(green_taxi_df_clean['lpep_dropoff_datetime'])\n",
    "\n",
    "green_taxi_df_clean = green_taxi_df_clean[(green_taxi_df_clean['lpep_pickup_datetime'].dt.year == 2018) & (green_taxi_df_clean['lpep_dropoff_datetime'].dt.year == 2018)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only trips that happen in may\n",
    "green_taxi_df_clean = green_taxi_df_clean[(green_taxi_df_clean['lpep_pickup_datetime'].dt.month == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what defines a trip? here we remove any \"trip\" that has no distance, no fare, or no time or negative time\n",
    "green_taxi_df_clean = green_taxi_df_clean[green_taxi_df_clean.fare_amount > 0]\n",
    "green_taxi_df_clean =green_taxi_df_clean[green_taxi_df_clean.trip_distance > 0]\n",
    "green_taxi_df_clean = green_taxi_df_clean[green_taxi_df_clean.lpep_pickup_datetime < green_taxi_df_clean.lpep_dropoff_datetime]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean = green_taxi_df_clean[green_taxi_df_clean.passenger_count <= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing and handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observing and handling placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean = green_taxi_df_clean.drop(['ehail_fee', 'congestion_surcharge'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the PU Location and DO Location to boroughs and zones\n",
    "\n",
    "\n",
    "\n",
    "def extract_location_info(df):\n",
    "    # Split PU Location and DO Location into borough and zone\n",
    "    df[\"pu_location_borough\"] = df[\"pu_location\"].str.split(',').str[0]\n",
    "    df[\"pu_location_zone\"] = df[\"pu_location\"].str.split(',').str[1]\n",
    "    df[\"do_location_borough\"] = df[\"do_location\"].str.split(',').str[0]\n",
    "    df[\"do_location_zone\"] = df[\"do_location\"].str.split(',').str[1]\n",
    "    return df\n",
    "\n",
    "# Call the function to add borough and zone columns to your DataFrame\n",
    "\n",
    "green_taxi_df_clean = extract_location_info(green_taxi_df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean = green_taxi_df_clean[(green_taxi_df_clean[\"pu_location_borough\"]!= \"Unknown\") &(green_taxi_df_clean[\"do_location_borough\"]!= \"Unknown\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if payment type is unknown and tip amount is 0 then payment type is cash\n",
    "#if payment type is unknown and tip amount is not 0 then payment type is credit card\n",
    "#also handle nulls\n",
    "def replace_unknown_payment_type(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isnull(row['payment_type']) or row['payment_type'] == 'Uknown':\n",
    "            if row['tip_amount'] == 0:\n",
    "                df.at[index, 'payment_type'] = 'Cash'\n",
    "            else:\n",
    "                df.at[index, 'payment_type'] = 'Credit card'\n",
    "    return df\n",
    "\n",
    "green_taxi_df_clean = replace_unknown_payment_type(green_taxi_df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean[\"extra_imp\"] = green_taxi_df_clean[\"extra\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers = green_taxi_df_clean[green_taxi_df_clean.passenger_count <= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers[\"passenger_count\"] = green_taxi_df_clean_outliers[\"passenger_count\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers = green_taxi_df_clean_outliers[green_taxi_df_clean_outliers['trip_distance'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_trip_distance_outliers(df):\n",
    "    # Calculate Z-scores for the specified column\n",
    "    z = np.abs(stats.zscore(green_taxi_df_clean['trip_distance']))\n",
    "    filtered_entries = z < 3.5\n",
    "    \n",
    "    # Impute outliers with the maximum value within Z-score threshold\n",
    "    max_within_threshold = df[filtered_entries][\"trip_distance\"].max()\n",
    "    df[\"trip_distance_imputed\"] = df[\"trip_distance\"]\n",
    "    df.loc[~filtered_entries, \"trip_distance_imputed\"] = max_within_threshold\n",
    "    \n",
    "    return df\n",
    "green_taxi_df_clean_outliers = handle_trip_distance_outliers(green_taxi_df_clean_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_fare_amount_outliers(df):\n",
    "    # Calculate Z-scores for the specified column\n",
    "    z = np.abs(stats.zscore(green_taxi_df_clean['fare_amount']))\n",
    "    filtered_entries = z < 3.5\n",
    "    \n",
    "    # Impute outliers with the maximum value within Z-score threshold\n",
    "    max_within_threshold = df[filtered_entries][\"fare_amount\"].max()\n",
    "    df[\"fare_amount_imputed\"] = df[\"fare_amount\"]\n",
    "    df.loc[~filtered_entries, \"fare_amount_imputed\"] = max_within_threshold\n",
    "    \n",
    "    return df\n",
    "green_taxi_df_clean_outliers = handle_fare_amount_outliers(green_taxi_df_clean_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tip_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tip_amount_iqr(df):\n",
    "    # Calculate the IQR (Interquartile Range)\n",
    "    Q1 = df[\"tip_amount\"].quantile(0.25)\n",
    "    Q3 = df[\"tip_amount\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define lower and upper bounds for outliers\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    # Identify outliers based on IQR bounds\n",
    "    outliers_mask = (df[\"tip_amount\"] > upper_bound)\n",
    "    \n",
    "    # Impute outliers with the upper limit of the IQR\n",
    "    upper_limit = Q3 + 3 * IQR\n",
    "    df.loc[outliers_mask, \"tip_amount\"] = upper_limit\n",
    "    \n",
    "    return df\n",
    "green_taxi_df_clean_outliers = handle_tip_amount_iqr(green_taxi_df_clean_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tolls_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tolls_amount(df):\n",
    "    # Calculate the IQR (Interquartile Range)\n",
    "    \n",
    "    # Define lower and upper bounds for outliers\n",
    "    \n",
    "    # Identify outliers based on IQR bounds\n",
    "    outliers_mask = (df[\"tolls_amount\"] > 5.76)\n",
    "    \n",
    "    # Impute outliers with the upper limit of the range containing majority of values\n",
    "    upper_limit = 5.76\n",
    "    df.loc[outliers_mask, \"tolls_amount\"] = upper_limit\n",
    "    \n",
    "    return df\n",
    "\n",
    "green_taxi_df_clean_outliers = handle_tolls_amount(green_taxi_df_clean_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_total_amount_outliers(df):\n",
    "    # Calculate Z-scores for the specified column\n",
    "    z = np.abs(stats.zscore(green_taxi_df_clean['total_amount']))\n",
    "    filtered_entries = z < 3.5\n",
    "    \n",
    "    # Impute outliers with the maximum value within Z-score threshold\n",
    "    max_within_threshold = df[filtered_entries][\"total_amount\"].max()\n",
    "    df.loc[~filtered_entries, \"total_amount\"] = max_within_threshold\n",
    "    \n",
    "    return df\n",
    "green_taxi_df_clean_outliers = handle_total_amount_outliers(green_taxi_df_clean_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Data transformation and feature eng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers_engineered = green_taxi_df_clean_outliers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_features(df):\n",
    "    # Create 'week_number' column\n",
    "    df['week_number'] = df[\"lpep_pickup_datetime\"].dt.isocalendar().week\n",
    "    \n",
    "    # Create 'date_range' column\n",
    "    df['date_range'] = df[\"lpep_pickup_datetime\"].dt.to_period('W')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "green_taxi_df_clean_outliers_engineered = generate_date_features(green_taxi_df_clean_outliers_engineered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Adding more features(feature eng.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_features(df):\n",
    "    # Create 'is_weekend' column (1 for weekend, 0 for weekday)\n",
    "    df['is_weekend'] = df[\"lpep_pickup_datetime\"].dt.dayofweek // 5 == 1\n",
    "    \n",
    "    # Create 'is_night' column (1 for night, 0 for day)\n",
    "    df['is_night'] = (df[\"lpep_pickup_datetime\"].dt.hour < 6) | (df[\"lpep_pickup_datetime\"].dt.hour >= 20)\n",
    "    \n",
    "    return df\n",
    "green_taxi_df_clean_outliers_engineered = generate_time_features(green_taxi_df_clean_outliers_engineered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers_engineered[\"pick_up_hour\"] = pd.to_datetime(green_taxi_df_clean_outliers_engineered[\"lpep_pickup_datetime\"]).dt.hour\n",
    "green_taxi_df_clean_outliers_engineered[\"drop_off_hour\"] = pd.to_datetime(green_taxi_df_clean_outliers_engineered[\"lpep_dropoff_datetime\"]).dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_features(df):\n",
    "    result = df.copy() # take a copy of the dataframe\n",
    "    label_encoding_columns = ['store_and_fwd_flag']\n",
    "    one_hot_encoding_columns = ['vendor',\"rate_type\", \"payment_type\", 'trip_type']\n",
    "    # apply label encoding to the specified columns\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    for col in label_encoding_columns:\n",
    "        result[col] = label_encoder.fit_transform(result[col])\n",
    "\n",
    "    # apply one-hot encoding to the specified columns using get_dummies\n",
    "    one_hot_encoding_df = pd.get_dummies(result[one_hot_encoding_columns])\n",
    "    \n",
    "    result = pd.concat([result, one_hot_encoding_df], axis=1)\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n",
    "green_taxi_df_clean_outliers_engineered_encoded = encode_features(green_taxi_df_clean_outliers_engineered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df):\n",
    "    result = df.copy() # take a copy of the dataframe\n",
    "    columns_to_scale = [\"trip_distance\",\"fare_amount\",\"tip_amount\",\"total_amount\"]\n",
    "    # apply min-max scaling to the specified columns\n",
    "    scaler = MinMaxScaler()\n",
    "    result[columns_to_scale] = scaler.fit_transform(result[columns_to_scale])\n",
    "    \n",
    "    return result\n",
    "\n",
    "green_taxi_df_clean_outliers_engineered_encoded_scaled = scale_features(green_taxi_df_clean_outliers_engineered_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Additional data extraction (GPS coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opencage.geocoder import OpenCageGeocode\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual OpenCage Geocoding API key\n",
    "api_key = \"e5370232998a4369b116891cd3297584\"\n",
    "\n",
    "def get_unique_coordinates(unique_locations):\n",
    "    geocoder = OpenCageGeocode(api_key)\n",
    "    coordinates_dict = {}\n",
    "\n",
    "    for location in unique_locations:\n",
    "        results = geocoder.geocode(location)\n",
    "        if results and len(results):\n",
    "            coordinates_dict[location] = {\n",
    "                'latitude': results[0]['geometry']['lat'],\n",
    "                'longitude': results[0]['geometry']['lng']\n",
    "            }\n",
    "\n",
    "    return coordinates_dict\n",
    "\n",
    "\n",
    "def add_coordinates_to_dataframe(df):\n",
    "    unique_pickup_locations = df[\"pu_location\"].unique()\n",
    "    unique_dropoff_locations = df[\"do_location\"].unique()\n",
    "    unique_locations = np.unique(np.concatenate((unique_pickup_locations, unique_dropoff_locations), axis=0))\n",
    "\n",
    "\n",
    "    unique_coordinates = get_unique_coordinates(unique_locations)\n",
    "    # dropoff_coordinates = get_unique_coordinates(unique_dropoff_locations)\n",
    "\n",
    "    df['pu_latitude'] = df[\"pu_location\"].map(lambda x: unique_coordinates.get(x, {}).get('latitude'))\n",
    "    df['pu_longitude'] = df[\"pu_location\"].map(lambda x: unique_coordinates.get(x, {}).get('longitude'))\n",
    "    df['do_latitude'] = df[\"do_location\"].map(lambda x: unique_coordinates.get(x, {}).get('latitude'))\n",
    "    df['do_longitude'] = df[\"do_location\"].map(lambda x: unique_coordinates.get(x, {}).get('longitude'))\n",
    "\n",
    "    return df\n",
    "\n",
    "green_taxi_df_clean_outliers_engineered_encoded_scaled_with_coordinates = add_coordinates_to_dataframe(green_taxi_df_clean_outliers_engineered_encoded_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers_engineered_encoded_scaled_with_coordinates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Exporting the dataframe to a csv file or parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df_clean_outliers_engineered_encoded_scaled_with_coordinates.to_csv('../dataset/cleaned_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
